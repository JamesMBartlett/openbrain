\section{Experimentation}
\subsection{Experiment 1: Learning Q functions on components of the Actor network.}
\textbf{Motivation:} 
Our first batch of experiments aimed to establish empirically that the Q-functions of each layer are similar to the Q-function of the overall network. To that end, we extended the actor-critic methodology used in Lillicrap et al (2016) as follows. In addition to  having one critic network estimating the Q-gradients for the entire network, sub-critic networks were initialized for each individual layer. The subcritic corresponding to a given layer was responsible for the gradient estimates of that given layer. \\
\newline
The inputs used in the calculation of the Q-functions for each layer follow the conceptual framework laid out in Section 2.2. The state and action parameters are therefore voltages represented by the outputs of $\epsilon : \scripts \to \scriptv$ and $\delta : \scriptv \to \scripta$ respectively, where the changes in voltage are described by a voltage transition function $K : \scriptv \to \scriptv$. \\
\newline
After [number of iterations], each $Q^n(s,a)$  networks effectively learned the same cost function as $Q^{\mu}(s,a)$.
\todo[inline]{Incorporate metrics regarding the performance $Q_n$ w.r.t $Q^{\mu}$'s weights}. Furthermore, the $Q^n$ plots of \todo[inline]{function name} correlate directly with the same plot for $Q^{\mu}$. This confirms that we are able teach the $Q^n$ model on the level of subcomponents of $\mu$ supporting the argument that we can use $Q^n$ nets to approximate $Q^\mu$. We can now use each $Q^n$ to calculate the gradient of the specific neuron component that the $Q^n$ is critiquing. We proceed with this in Experiment 2.
