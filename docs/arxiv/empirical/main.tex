% In your .tex file

\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{xparse}

\usepackage{hyperref}
\usepackage{url}
\usepackage[toc,page]{appendix}

%citation
%\usepackage[backend=bibtex]{biblatex}
\bibliography{openbrain}
%

% tikz and associated macros
\usepackage{tikz}
\usepackage{tikz-cd}

\usepackage{pgfplots}
\newcommand\sep{1.9cm}
\newcommand\height{0.9cm}
\usetikzlibrary{decorations.pathmorphing, backgrounds}
\tikzset{snake it/.style={decorate, decoration=snake}}
%
%

% math
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}

\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  % end of proof
\newtheorem{example}{Example} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{proposition}[theorem]{Proposition} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}

\numberwithin{equation}{subsection}
\numberwithin{theorem}{subsection}

\DeclareSymbolFont{cmlargesymbols}{OMX}{cmex}{m}{n}
\let\sumop\relax
\DeclareMathSymbol{\sumop}{\mathop}{cmlargesymbols}{"50}
\NewDocumentCommand{\tens}{t_}
 {%
  \IfBooleanTF{#1}
   {\tensop}
   {\otimes}%
 }
\NewDocumentCommand{\tensop}{m}
 {%
  \mathbin{\mathop{\otimes}\displaylimits_{#1}}%
 }


\def\reals{{\mathbb R}}
\def\torus{{\mathbb T}}
\def\integers{{\mathbb Z}}
\def\rationals{{\mathbb Q}}
\def\expect{\mathop{{\mathbb{E}}}}
\def\naturals{{\mathbb N}}
\def\complex{{\mathbb C}\/}
\def\distance{\operatorname{distance}\,}
\def\support{\operatorname{support}\,}
\def\dist{\operatorname{dist}\,}
\def\Span{\operatorname{span}\,}
\def\degree{\operatorname{degree}\,}
\def\kernel{\operatorname{kernel}\,}
\def\dim{\operatorname{dim}\,}
\def\codim{\operatorname{codim}}
\def\trace{\operatorname{trace\,}}
\def\dimension{\operatorname{dimension}\,}
\def\codimension{\operatorname{codimension}\,}
\def\kernel{\operatorname{Ker}}
\def\Re{\operatorname{Re\,} }
\def\Im{\operatorname{Im\,} }
\def\eps{\varepsilon}
\def\lt{L^2}
\def\bull{$\bullet$\ }
\def\det{\operatorname{det}}
\def\Det{\operatorname{Det}}
\def\diameter{\operatorname{diameter}}
\def\symdif{\,\Delta\,}
\newcommand{\norm}[1]{ \|  #1 \|}
\newcommand{\set}[1]{ \left\{ #1 \right\} }
\def\suchthat{\mathrel{}\middle|\mathrel{}}
\def\one{{\mathbf 1}}
\def\cl{\text{cl}}

\def\newbull{\medskip\noindent $\bullet$\ }
\def\nobull{\noindent$\bullet$\ }
\def\defeq{\stackrel{\text{def}}{=}}


\def\scriptf{{\mathcal F}}
\def\scriptq{{\mathcal Q}}
\def\scriptg{{\mathcal G}}
\def\scriptm{{\mathcal M}}
\def\scriptb{{\mathcal B}}
\def\scriptc{{\mathcal C}}
\def\scriptt{{\mathcal T}}
\def\scripti{{\mathcal I}}
\def\scripte{{\mathcal E}}
\def\scriptv{{\mathcal V}}
\def\scriptw{{\mathcal W}}
\def\scriptu{{\mathcal U}}
\def\scriptS{{\mathcal S}}
\def\scripta{{\mathcal A}}
\def\scriptr{{\mathcal R}}
\def\scripto{{\mathcal O}}
\def\scripth{{\mathcal H}}
\def\scriptd{{\mathcal D}}
\def\scriptl{{\mathcal L}}
\def\scriptn{{\mathcal N}}
\def\scriptp{{\mathcal P}}
\def\scriptk{{\mathcal K}}
\def\scriptP{{\mathcal P}}
\def\scriptj{{\mathcal J}}
\def\scriptz{{\mathcal Z}}
\def\scripts{{\mathcal S}}
\def\scriptx{{\mathcal X}}
\def\scripty{{\mathcal Y}}
\def\frakv{{\mathfrak V}}
\def\frakG{{\mathfrak G}}
\def\frakB{{\mathfrak B}}
\def\frakC{{\mathfrak C}}



%\todo[inline]v %NOTES. To remove for camera ready version.
\usepackage{todonotes}
\usepackage{regexpatch}
%end to notes

\title{Backpropagation-Free Parallel Deep Reinforcement Learning}

\author{
William H.~Guss \\
Machine Learning at Berkeley\\
2650 Durant Ave, Berkeley CA, 94720 \\
\texttt{wguss@ml.berkeley.edu} \\
\And
Mike Zhong \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{lol@gmail.com} \\
\And
Utkarsh S \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{philkuz@ml.berkeley.edu}
\And
Max Johansen \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{max@ml.berkeley.edu}
}



\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
	In this paper we conjecture that an agent, envirionment pair $(\pi, E)$ trained using DDPG with an actor network $\mu$ and critic network $Q^{\pi}$ can be decomposed into a number of sub-agent, sub-environment pairs  $(\pi_n, E_n)$ ranging over every neuron in $\mu$; that is, we show empircially that treating each neuron $n$ as an agent $\pi_n: \mathbb{R}^n \to \mathbb{R}$ of its inputs and optimizing a value function $Q^{\pi_n}$ with respect to the weights of $\pi_n$ is dual to optimizing $Q^\pi$ with respect to the weights of $\mu$. Finally we propose a learning rule which simultaneously optimizes each $\pi_n$ without error backpropogation achieving state of the art performance and speed across a variety of OpenAI Gym environments.
\end{abstract}
\listoftodos


\section{Introduction}
\todo[inline]{Introduction to DDPG and recent advances in deep RL. } 

\todo[inline]{Biological diffusion of dopamine in the brain$\implies$ error backpropagation is not biologically feasible.} 

\todo[inline]{Synthetic gradients are a step in the right direction, but still require eventual back propogation.} 

\todo[inline]{Therefore it is feasible that each neuron is maximizing the expectation on his future dopamine intake, and so we propose the following theorem.  } 

\section{Agent-Environment Value Decomposition}

\todo[inline]{A high level description of the section.}

\subsection{Background}
Recall the standard reinforcement learning setup. We say $E$ is an \emph{environment} if $E \defeq (\scripts, \scripta, \scriptr, T, r)$ where $T$ describes transition probability measure $T\left(s_{t+1}\suchthat s_t, a_t\right)$ and $r: \scripts \times \scripta \to \scriptr$ is a reward function. Furthermore $\scripts$, $\scripta$, $\scriptr$ are the \emph{state space, action space, and reward space} respectively. We restrict $\scriptr$ to a compact subset of $\mathbb{R}$ and action space and state space to finite dimensional real vector spaces. As in DDPG we assume that the environment $E$ is \emph{fully observed}; that is, at any time step the state $s_t$ is fully described by the observation presented, $x_t$, and not by the history $(x_1, a_1, \dots, a_{t-1}).$ 

We define the policy for an agent to be $\pi: \scriptp(\scripta) \times \scripts \to [0,1]$. In general the policy is a probability measure on some $\sigma$-algebra $\scriptm \subset \scriptp(\scripta)$ conditioned on $\scripts$ so that $\pi\left(\scripta \suchthat s \in \scripts\right) = 1$. However, we will deal only with \emph{deterministic} policies where for every $s_t$ there is unique $a_t$ so that $\pi\left(\set{a_t} \suchthat s = s_t\right) = 1$ and the measure is $0$ otherwise. Thus we will abuse notation and define a \emph{deterministic agent} by a policy function $\pi: \scripts \to \scripta$.	

For a policy $\pi$  the action-value function is the expected future reward under $\pi$ by performing $a_t$ at state $s_t$ using the Bellman equation
\begin{equation}
	Q^{\pi}(s_t, a_t) = \expect_{s_{t+1} \sim E}\left[r(s_{t}, a_t) + \gamma Q^{\pi}(s_{t+1}, \pi(s_{t+1}))\right]
\end{equation}
with $\gamma \in (0,1)$ a discount factor, and the second expectation removed because $\pi$ is deterministic. \todo{Insert reference and make this a footnote}{[Some survey]} provides an extensive exposition into a justification of this equation and choice for the action-value of $\pi$, so we will assume such a choice is a valid measure of performance.

In deterministic policy gradient methods, we define an actor $\mu: \scripts \to \scripta$ and a critic $Q^\mu: \scripts \times \scripta \to \mathbb{R}$ and optimize $Q^\mu(s_t, \mu(s_t)$ with respect to the paramers $\theta^\mu$ of $\mu.$ This method is provably the true policy gradient of $\mu$ if $Q^\mu$ is know. Recently \todo{Cite lilicrap}{(DDPG)} utilizes the universality of DNNs in order to approximate both $\mu$ and $Q^\mu$ along with delayed weight-transfer networks to stabilize learning and  prevent divergence as depicted in \todo{Make DDPG figure}{Figure 1}. In order to decompose the action-value function we will make heavy use of this methodology at a scale local to each neuron in the flavor of \todo{Cite deepmind}{(Synthetic gradients.)}

\subsection{Towards Neurocomputational Decomposition of $Q^\mu$}

In order to decompose the $Q^\mu$ algorithm we will abstractly define a neurocomputational agent in terms of an operator on voltages with no restrictions on the topology of the network, and then relate the action-value function of the whole agent to those which are dfined for each individual neuron in the network.

If $\scriptv$ is an $N$-dimensional vector space then a \emph{neurocomputational agent} is a tuple $\scriptn = (\mu, \epsilon, \delta, K, \Theta, \sigma, V)$ such that: 
\begin{itemize}
	\item $\epsilon : \scripts  \to N_I \subset \scriptv$ encodes the state into the voltages of \emph{input neurons}, a subspace $N_I$ of the voltages $\scriptv\subset \mathbb{R}^N$ of every neuron in the network. Specifically $\epsilon(s_t) = \mathrm{proj}_{N_I}(s_t)$.
	\item $\delta: \scriptv \to \scripta$ decodes the voltages of the \emph{output neurons }$N_O \subset V$ into an action.
	\item $K: \scriptv \to \scriptv$ is the linear voltage graph transition function of the graph representing the topolopy of $\scriptn$, parameterized by $\theta$.
	\item $\Theta: \scriptv \to \scriptv$ is a nonlinear inhibition function.
	\item $\sigma: \scriptv \to \scriptv$ is the elementwise application of some activation function to the voltage vector.
	\item $V: \mathbb{N} \to \scriptv$ is the voltage of $\scriptn$ at a discrete internal timestep $\tau$ such that
	\begin{equation}
		\begin{aligned}
			V(\tau+1) \defeq \sigma\left(K\Theta[V(\tau)]\right) + I(\tau),&\;\;\;\;\;\;\;\;\;\;\;&	V(0) \defeq 0.
		\end{aligned}
	\end{equation}
	where $I(\tau)$ is some input function.
	\item $\mu: \scripts \to \scripta$ is the deterministic policy for $\scriptn$. 
For some agents, the internal time $\tau$ is not in sync with $t$. For example if $\scriptn$ standard $\ell$ layer DNN, then the policy decodes a voltage after an $\ell$ step delay; that is, if $s_t$ observed at $\tau$, then $\mu(s_t) = V(\tau +\ell)$, and $I(\tau) = \epsilon(s_t)$, but $I(\tau +n) =0$ when $n \leq \ell$.
\end{itemize}
It is not hard to see that this definition encompasses any DQN or DDPG network with either reccurent or non recurrent layers. Additionally other paradigms such as the leaky integrattor are neurocomputational agents. (See appendix.)

Now let $E$ and $\scriptn$ be defined as above. If $n$ is some neuron in $\scriptn$ with input and output subspaces $X,Y \subset \mathcal{V}$, then define the a deterministic \emph{sub-environment} $E^n = (\scriptv, \mathbb{R}, \scriptr, T^n, r^n)$. In this case the transition describes how the voltage of $n$ affects the voltage of $\scriptn$ that is, $T\left(V' \suchthat V(\tau) \in \scriptv, a_\tau\right) = 1$ where $V'$ is the voltage of $\scriptn$ at the next time step if $V(\tau)_n \defeq a_\tau$ (disregardin ghe defintion of $V(\tau)$. The reward function is $r(V(\tau +\ell), a_{\ell + n}) = r_t(s_t, \mu(s_t))$ if $s_t$ presented at $\tau$ and $\mu(s_t)$ decodes $\scriptn$ at time $\tau + n$, otherwise $r(v,a) = 0.$

Finally define an agent $\mu^n$ 
\todo[inline]{Build up definitions required }
 
\todo[inline]{Write conjecture on decomposition which is free of neural configuration. Subject to change in later versions of ArXiv paper} 

\todo[inline]{Emperical justification of the iff using the following experiment (s).} 
\todo[inline]{1. Training a network on Atari using DDPG and plotting average critic functions for neurons using window.} 
\todo[inline]{2. Possibly others.} 


\todo[inline]{Therefore we propose the following learning rule in aims to evidence the reverse, training $\mu$ using simultaneous optimization on all $Q_n$ w.r.t $\pi_n$'s weights.
} 

\section{Decentralized Deep Determinstic Policy Gradient Learning}
\todo[inline]{Proposal of the rule. Linear approximation of the $Q$ function for every neuron is good enough, (experimentally).} 

\todo[inline]{Implications of the rule to DDPG}

\todo[inline]{Implications of the rule to entirely recurrent networks (infinite time horizion and NO unrolling since the environment the local actions of the neuron which globally recur to that neuron again are \emph{encoded} into $Q_n$; large time horizion probably implies that better regresser needed for $Q_n$.)} 

\todo[inline]{Parallelism, no error backprop, and only 2x operations, but no locking on GPU, so all can be run sumultaneously if we cache!} 


\section{Results}
\todo[inline]{To validate the new learning rule we throw a fuck ton of experiments together on the following list (or better using OpenAI Gym).} 
\begin{verbatim}
blockworld1 1.156 1.511 0.466 1.299 -0.080 1.260
blockworld3da 0.340 0.705 0.889 2.225 -0.139 0.658
canada 0.303 1.735 0.176 0.688 0.125 1.157
canada2d 0.400 0.978 -0.285 0.119 -0.045 0.701
cart 0.938 1.336 1.096 1.258 0.343 1.216
cartpole 0.844 1.115 0.482 1.138 0.244 0.755
cartpoleBalance 0.951 1.000 0.335 0.996 -0.468 0.528
cartpoleParallelDouble 0.549 0.900 0.188 0.323 0.197 0.572
cartpoleSerialDouble 0.272 0.719 0.195 0.642 0.143 0.701
cartpoleSerialTriple 0.736 0.946 0.412 0.427 0.583 0.942
cheetah 0.903 1.206 0.457 0.792 -0.008 0.425
fixedReacher 0.849 1.021 0.693 0.981 0.259 0.927
fixedReacherDouble 0.924 0.996 0.872 0.943 0.290 0.995
fixedReacherSingle 0.954 1.000 0.827 0.995 0.620 0.999
gripper 0.655 0.972 0.406 0.790 0.461 0.816
gripperRandom 0.618 0.937 0.082 0.791 0.557 0.808
hardCheetah 1.311 1.990 1.204 1.431 -0.031 1.411
hopper 0.676 0.936 0.112 0.924 0.078 0.917
hyq 0.416 0.722 0.234 0.672 0.198 0.618
movingGripper 0.474 0.936 0.480 0.644 0.416 0.805
pendulum 0.946 1.021 0.663 1.055 0.099 0.951
reacher 0.720 0.987 0.194 0.878 0.231 0.953
reacher3daFixedTarget 0.585 0.943 0.453 0.922 0.204 0.631
reacher3daRandomTarget 0.467 0.739 0.374 0.735 -0.046 0.158
reacherSingle 0.981 1.102 1.000 1.083 1.010 1.083
walker2d 0.705 1.573 0.944 1.476 0.393 1.397
\end{verbatim}
\todo[inline]{1. Show that training decentralized policy gradient $\implies$ total policy optimization }

\todo[inline]{2. Show speed improvements on update step through parallelism (samples per second vs DDPG). }

\todo[inline]{3. Show results are comparable with the state of the art.}

\section{Conclusion}
\todo[inline]{We wrecked deep reinforcement learning using biological inspiration. }
\subsection{Future Work}
\todo[inline]{Would like to try the method with full recurrent networks and purely asynchronous implementation of leaky integration networks.}
\todo[inline]{Would like to prove the conjecture. List possible methods of proof.}

%\listoftodos

%\printbibliography

%\input{appendices}

\end{document}	