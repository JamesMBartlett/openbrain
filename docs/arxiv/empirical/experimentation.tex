\section{Experimentation}
\subsection{Experiment 1: Learning Q functions on components of the Actor network.}


\subsection{Experiment 2: Treating each neuron as its Actor-Critic network using linear approximators}

Now that we have established that the Critic networks for each of the individual neuron learns the same Q-function as does the entire agent, we now treat each neuron as its own actor. Each neuron changes the weights of its presynaptic neurons' connections, as parameters for its actor -- its voltage on the next timestep -- to optimize its learned approximated Q function. We use the linear approximation:

$$Q^{n}(v, a) \approx \theta_{n,v}^T v + \theta_{n,a}a = \theta_{n}^T (v, a)^T$$
$$\mu^{n}(v) = \sigma(K^n v)$$

GOT RID OF $\Theta$ in that it's not very necessary ...

The algorithm for learning (very much INSPIRED by [CONTINUOUS CONTROL WITH DEEP REINFORCEMENT
LEARNING]):

For each neuron n:
\begin{enumerate}
\item[] Initialize random weights $\theta_{n,v}$, $\theta_{n,a}$, and $K^n$
\item[] Initialize target network $Q^n'$ and $\mu^n'$ with same weights, $\theta'_{n,v}$, $\theta'_{n,a}$, and $K'^n$
\item[] Initialize replay buffer R
\item[] For each episode:
\begin{enumerate}
\item[] Receive initial observation voltages $v_1$
\item[] for t=1, T, do:
\begin{enumerate}
\item[] follow dynamics, $a^n_{t+1} = \sigma(K^n v_t)$
\item[] Observe reward $r_t$, observe new voltages $v_{t+1}$
\item[] Store transition $(v_t, a_t, r_t, v_{t+1})$ in R.
\item[] Sample a random minibatch of $N$ transitions $(v_i, a_i, r_i, v_{i+1})$ from R
\item[] Set $y_i = r_i + \gamma Q'(v_{i+1}, \mu'(v_{i+1}))$
\item[] Update critic weights $\theta_n$ by minimizing loss $L := \frac{1}{N} \sum_i (y_i - Q(v_i, a_i))^2$
\item[] Update actor weights (connections) using the sample policy gradient:
$$\nabla_{K^n} J \approx \frac{1}{N} \sum_i \nabla_a Q(v, a) \vert_{v=v_i, a=\mu^n(v_i)} \nabla_{K^n} \mu^n(v) \vert_{v_i}$$
\item[] update the target networks:
$$\theta_n' \leftarrow \tau \theta_n + (1 - \tau) \theta_n'$$
$$K^n' \leftarrow \tau K^n + (1 - \tau) K^n'$$
\end{enumerate}
\item[] end for
\end{enumerate}
\item[] end for
\end{enumerate}

We train the critic weights $(\theta_{n,v}, \theta_{n,a})$ by minimizing the loss:

$$L = \frac{1}{2}((r_i + \gamma Q^n'(v_{i+1}, a_{i+1})) -  Q(v_{i}, a_{i})) \\
= \frac{1}{2}D_i^2$$


where $D_i := (r_i + \gamma \theta_n^T' (v_{i+1}, a_{i+1})) - \theta_n (v_{i}, a_{i}) $

We thus have:
$$\nabla_{\theta_n} L = -D_i (v_{i}, a_{i})$$

or, using gradient descent with learning rate $\eta_Q$, we have:
$$\theta_n \mathrel{-}= \eta_Q \nabla_{\theta_n} L = -\eta_Q D_i (v_{i}, a_{i})$$

or:

$$\theta_{n,v} \mathrel{+}= \eta_Q D_i v_i$$
and
$$\theta_{n,a} \mathrel{+}= \eta_Q D_i a_i$$

As for updating the actor policy, with learning rate $eta_A$, we have:
$$K^n \mathrel{+}= \eta_A \nabla_a Q(v, a) \vert_{v=v_i, a=\mu^n(v_i)} \nabla_{K^n} \mu^n(v) \vert_{v_i} = \eta_A \theta_{n,a} \sigma'(K^n v_i) v_i$$

IN CONCLUSION:

$$\theta_{n,v} \mathrel{+}= \frac{1}{N} \sum_i \eta_Q D_i v_i$$
$$\theta_{n,a} \mathrel{+}= \frac{1}{N} \sum_i \eta_Q D_i a_i$$
$$K^n \mathrel{+}= \frac{1}{N} \sum_i \eta_A \theta_{n,a} \sigma'(K^n v_i) v_i$$

where $D_i := (r_i + \gamma \theta_n^T' (v_{i+1}, a_{i+1})) - \theta_n (v_{i}, a_{i}) $
