\section{Experimentation}
\subsection{Experiment 1: Learning Q functions on components of the Actor network.}
\textbf{Motivation:} 
Our first batch of experiments aimed to establish empirically that the Q-functions of each layer, denoted $Q_1...Q_L$ are similar to the Q-function of the overall network, denoted $Q_{\mu}$. To that end, we extended the actor-critic methodology used in Lillicrap et al (2016) as follows. In addition to  having one critic network estimating the Q-gradients for the entire network, sub-critic networks were initialized for layers. The subcritic corresponding to a given layer was responsible for the Q-function estimates of that given layer. \\
\newline
The inputs used in the calculation of the Q-functions for each layer follow the conceptual framework laid out in Section 2.2. The state and action parameters are therefore voltages represented by the outputs of $\epsilon : \scripts \to \scriptv$ and $\delta : \scriptv \to \scripta$ respectively, where the changes in voltage are described by a voltage transition function $K : \scriptv \to \scriptv$. \\ 
\newline
To determine and compare the rate at which the subcritics and the main critic learn, the experiment was run in two phases. First, each of the subcritics and the main critic were trained using the standard DDPG algorithm on some actor $\mu$. In the second phase, a new actor $\mu'$ was initialized and its Q-function set to $Q_{\mu}$ as determined above in phase 1. The subcritics were trained, and the values of $Q_1\dots Q_{L}$ were plotted as training occurred. \textbf{TODO: Introduce the notion of similary used to compare the q-functions.}
\newline
After [number of iterations], each $Q^n(s,a)$  networks effectively learned the same cost function as $Q^{\mu}(s,a)$.
\todo[inline]{Incorporate metrics regarding the performance $Q_n$ w.r.t $Q^{\mu}$'s weights}. Furthermore, the $Q^n$ plots of \todo[inline]{function name} correlate directly with the same plot for $Q^{\mu}$. This confirms that we are able teach the $Q^n$ model on the level of subcomponents of $\mu$ supporting the argument that we can use $Q^n$ nets to approximate $Q^\mu$. We can now use each $Q^n$ to calculate the gradient of the specific neuron component that the $Q^n$ is critiquing. We proceed with this in Experiment 2.
