%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                      Homework _                            %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letter]{article}

\usepackage{lipsum}
\usepackage[pdftex]{graphicx}
\usepackage[margin=1.5in]{geometry}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{titling}
\usepackage{fancyhdr}

\pagestyle{fancy}


\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\newenvironment{menumerate}{%
  \edef\backupindent{\the\parindent}%
  \enumerate%
  \setlength{\parindent}{\backupindent}%
}{\endenumerate}







%%%%%%%%%%%%%%%
%% DOC INFO %%%
%%%%%%%%%%%%%%%

\title{RC Research Fellowship Proposal: OpenBrain}
\author{William Guss\\26793499\\wguss@berkeley.edu}

\fancyhead[L]{RC Research Fellowship}
\fancyhead[CO]{Proposal}
\fancyhead[CE]{GUSS}
\fancyhead[R]{\thepage}
\fancyfoot[LR]{}
\fancyfoot[C]{}
\usepackage{csquotes}

%%%%%%%%%%%%%%

\begin{document}
\maketitle
\thispagestyle{empty}


%%%%%%% Be sure to set the counter and use menumerate
\section{Background}
Machine learning research in the past decade has had a large focus on variations of the artificial neural network (ANN),
a biologically inspired algorithm originating from the mathematical neuron model proposed by McCulloch and Pitts\cite{mcpitts},
the perceptron neuron proposed by Rosenblatt\cite{perceptron}, and made practical by the back propogation algorithm created by Werbos \cite{bprop}.
Although derivative of biological neural networks, the ANN and its variations stray away from many features involved in biological learning. One
such feature is that learning occurs at the level of the individual biological neuron, whereas the ANN paradigm centralizes this process.
Additionally, ANN learning algorithms don't have a notion of synaptogenesis, which has been shown to be a key component of learning in the biological
brain\cite{gene}. Finally, there is the brain's distribution of processing among billions of individual neurons, something that is difficult to handle
in fully connected paradigms of ANN \cite{annbook}.

\section{Goals}
There are essentially four goals of the OpenBrain project.
\begin{itemize}
\item Build a massiveley parallel Beowulf cluster of parallela computers controlled using MPI on ArchLinux.
\item Create an always online, turing complete modification to the recursive neural network algorithm whose fitness
is determined by the Universal Intelligence Measure described in (Legg and Veness, 2011). The algorithm must have the
following constraints:
    \begin{itemize}
        \item In the spirit of John Conway's turing complete Game of Life \cite{conway}, the individual neural nodes must follow
         arbitrarily simple rules in a decentralized fashion.
        \item \emph{Training} is unsupervised and occurs over the lifetime of an \emph{instance} of the open brain,
        such that the aforementioned governing rules are modified with respect to the fitness of the instance.
    \end{itemize}
\item Implement each neural node as an Erlang process distributed across the Beowulf cluster asynchronously.
\item Provide always on input/output to the OpenBrain cluster in similar fashion to that done in Google DeepMind's
  Deep Reinforcement Learning.
\end{itemize}

\section{Plans}
The OpenBrain project will commense in two phases. First the algorithm must be precisely theorized and mathematical
guarentees about its capabilities must be developed. Then the project will implement the algorithm through the proper hardware and software.

For the theory behind OpenBrain, we apply the neuron model proposed in McCulloch and Pitts \cite{mcpitts} and assume that each neuron
functions according Conwaynian turing complete rules; that is in particular, we apply the synaptogenesis model suggested in
(Thomas et al, 2015) with parameters dictated by a universal intelligence indicator function.

For any neuron $\pi$ in the OpenBrain algorithm, a list of axonally connected posterior neurons $P_\pi$ is stored along with
a list of proximal neighboring neurons, $N_\pi.$ The neuron $\pi$ is efficiently implimented as asyncrhonous thread with a message
queue such that neurons connected to its anterior can \ activate and provide a voltage on $\pi$ such that $\pi$ itself
will activate. Furthermore, each neuron has the capacity to signal to a particular proximal neuron in order to form a new dendritic or axonal
synapse.

Under this lightweight model, we will experiment freely with different schemes of synaptogenesis and learning. Upon finding the most
appropriate model, guarentees will be made on its computational capacity of the algorithm, and thenthe second
phase of the project will commense.

The second phase involves implementing the algorithm on a massive parallel computing system, therefore the initial
focus of this phase is to construct a 32 node Beowulf cluster of Parallela computers. The necessity for this construction
is due to the large amount of CPU hours required to have the algorithm run continually, something that would not be cost effective
on clusters such as Berkeley's own PSI cluster. Furthermore, the ability to control the hardware of the project provides the opportunity
to maximize the performance of the algorithm with respect to the available processing power.

Once completed the child nodes of the cluster will be linked directly to a build system using hooks from Github which automatically pushes Erlang
OTP code for the OpenBrain algorithm to every node. This will allow for easy prototyping and utilizes Erlang's live code hot swapping feature.

Finally much research can be done in a variety of external environments using the universal intelligence indicator
and other metrics that evaluate performance. Specifically, OpenBrain will be compared to algorithms like Deep Reinforcement Learning.
These applications and comparisons will be understood with more specificity as the project matures.

\section{Qualifications}
\textbf{Coursework}
\begin{menumerate}
\item Honors Real Analysis (MATH H104) [A] - Useful tools for providing mathematical guarentees on computability.
\item Data Structures and Algorithms (CS61B) [A] - Useful for ensuring optimal computational complexity as it pertains to the project.
\end{menumerate}


\textbf{Past Research} \\
Over the past year, I've been developing an algorithmic generalization of McCulloch and Pitts feed-forward neural networks
called functional neural networks. The project has provided me the education and the toolset to give actuall guarentees on
a variety of different neural network algorithms. The project received a prize from the AAAI at the Intel International Science and Engineering Fair.
Attached is a copy of the paper, currently in progress, describing the project. Hopefully that may provide a sample of the quality
of research I will do.

\textbf{Miscelaneous} \\
Over the past semester I've attended Peter Bartlett's Statistical Machine Learning reading group weekly.

\section{Significance of the Project}
The aim of this project is to reapproach the biological model of the neural network through an alternative perspective. Through the treatment of
neurons as independent components, we aim to develop intelligence as an emergent phenomenon. Our method for implementing the algorithm utilizes
a novel architecture through the capabilities of erlang, specifically that the implementation will be entirely asynchronous. This provides
a number of benefits including the ability to use massively distributed computing and therefore easily scales. We hope that the discoveries found
through this project set the way for the development of a general artificial intelligence in the future.

\section{Budget}
The majority of the budget of this project deals directly with the construction of the Beowulf cluster.


\vspace{7mm}
\centering
\begin{tabular}{ |p{2cm}|p{6cm}|p{1.5cm}|p{1cm}| p{1.5cm}|
 }
 \hline
 \multicolumn{5}{|c|}{Budget} \\
 \hline
Item& Use& Quantity&Price & Total\\
 \hline
  Parallela & Used as the primary computation unit for the cluster.  Has 18 discrete cores and low power
  consumption.  & 32 & \$100  & \$3200\\
  \hline
  8 GB Class 10 SD Card & Storage for ArchLinux on each parallela node. &32 &\$8.79 &\$281.28 \\
    \hline
  Standoff M/F M3 25mm &Structureal support for nodes. &74 &\$0.40 &\$27.79 \\
    \hline
  2ft Cat6 Network Ethernet Patch Cable & For connecting the nodes to a network switch. &32 &\$2.11 &\$67.60 \\
   \hline
  Cisco SF200-48 Switch 10/100 & Network switch for placing all nodes on a subnet. &1 &\$283.53 &\$283.53 \\

 \hline
\end{tabular}
% \section
\begin{thebibliography}{1}
    \bibitem{annbook} Nicolaos Karayiannis and Anastasios N. Venetsanopoulos {\em
    Artificial Neural Networks: Learning Algorithms, Performance Evaluation, and Applications} 2013: Springer Science
    & Business Media.

    \bibitem{mcpitts} Warren S. McCulloch and Walter Pitts {\em A Logical Calculus of the Ideas Immanent in Nervous Activity} 1943.

    \bibitem{perceptron} Frank Rosenblatt {\em The perceptron: a probabilistic model for information storage and organization in the brain.}
    1958: Psychological review, 65(6), 386.

    \bibitem{bprop} Paul Werbos {\em Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences.} 1974: PhD thesis,
    Harvard University.

    \bibitem{sgene} Monica Hoyos Flight {\em Synaptogenesis: Switching to learn} 2011: Nature Review Neuroscience.

    \bibitem{conway} Martin Gardner {\em Mathematical Games – The fantastic combinations of John Conway's new solitaire game life} 1970:
     Scientific American 223 pp. 120–123

\end{thebibliography}

\end{document}
