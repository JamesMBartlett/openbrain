\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}

\usepackage{hyperref}
\usepackage{url}
\usepackage[toc,page]{appendix}

%citation
\usepackage[backend=bibtex]{biblatex}
\bibliography{openbrain}
%

% tikz and associated macros
\usepackage{tikz}

\usetikzlibrary{decorations.pathmorphing}
\tikzset{snake it/.style={decorate, decoration=snake}}
%

% math
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathabx}
\newcommand{\BlackBox}{\rule{1.5ex}{1.5ex}}  % end of proof
\newtheorem{example}{Example} 
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma} 
\newtheorem{proposition}[theorem]{Proposition} 
\newtheorem{remark}[theorem]{Remark}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{axiom}[theorem]{Axiom}

\numberwithin{equation}{subsection}
\numberwithin{theorem}{subsection}

\DeclareSymbolFont{cmlargesymbols}{OMX}{cmex}{m}{n}b
\let\sumop\relax
\DeclareMathSymbol{\sumop}{\mathop}{cmlargesymbols}{"50}

%

%% todo[inline] NOTES. To remove for camera ready version.
\usepackage{todonotes}
\usepackage{regexpatch}
%end to notes

\title{OpenBrain: Massively Asynchronous Neurocomputation}


\author{
William H.~Guss \\
Machine Learning at Berkeley\\
2650 Durant Ave, Berkeley CA, 94720 \\
\texttt{wguss@ml.berkeley.edu} \\
\And
Mike Zhong \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{lol@gmail.com} \\
\And
Phillip Kuznetsov \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{philkuz@ml.berkeley.edu} \\
\And
Jacky Liang \\
Machine Learning at Berkeley \\
Berkeley CA, 94720 \\
\texttt{jackyliang@berkeley.edu} \\
\And
Instert yourself \\
Machine Learning at Berkeley \\
Address \\
\texttt{email} \\
\And
We can add footnotes clarifying \\
Machine Learning at Berkeley \\
Address \\
\texttt{email} \\
\And
equal contributions to the work \\
(if needed)\\
Machine Learning at Berkeley \\
Address \\
\texttt{email} 
}


\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
	In this paper we introduce a new framework and philosophy for recurrent neurocomputation. By requiring that all neurons act asynchrynously and independently, we introduce a new metric for evaluating the universal intelligence of continuous time agents. We proved representation and universal approximation results in this context which lead to a set of learning rules in the spirt of John Conway's game of life. Finally evaluate this framework against different intelligent agent algorithms by implementing an approximate universal intelligence measure for agents embedded in turing computable environments in Minecraft, BF, and a variety of other reference machines. 
\end{abstract}


\listoftodos
\input{introduction}
\input{model}
\input{learning}
\input{implementation}
\input{conclusion}

\printbibliography

\input{appendices}

\end{document}